# FutureVox+ Default Configuration

# Paths
data_dir: "datasets/raw"
output_dir: "outputs"
checkpoint_dir: "${output_dir}/checkpoints"
log_dir: "${output_dir}/logs"
binary_dir: "${data_dir}/binary"    # Directory for binary dataset files

# Binary file settings
binary:
  enabled: true                    # Whether to use binary files instead of raw files
  train_file: "outputs/processed/binary/train.h5"
  val_file: "outputs/processed/binary/val.h5"
  test_file: "outputs/processed/binary/test.h5"
  validate_on_load: true           # Validate binary files during loading
  create_if_missing: true          # Create binary files if not found

# Model architecture config (reduced for single speaker)
model:
  name: "FutureVox+"
  
  # Speaker mixture module (reduced for single speaker)
  speaker_mixture:
    embedding_dim: 128
    num_speakers: 1
    hidden_dims: [128, 64, 32]
    attribute_net_dims: [64, 32, 16]
  
  # Language mixture module (reduced complexity)
  language_mixture:
    embedding_dim: 64
    num_languages: 1
    num_phonemes: 100
    hidden_dims: [128, 64, 32]
    accent_strength_dims: [64, 32, 16]
  
  # F0 processing module (reduced complexity)
  f0_processor:
    input_dim: 64
    hidden_dims: [64, 32, 16]
    num_registers: 2
    decomp_dims: [128, 64]
  
  # Acoustic feature generation (reduced complexity)
  acoustic_generator:
    model_dim: 128
    num_layers: 2
    num_heads: 2
    ffn_dim: 512
    dropout: 0.1
    cross_attention_layers: 1
  
  # Neural vocoder (slightly reduced complexity)
  vocoder:
    input_dim: 80
    hidden_dims: [256, 512, 1024]
    upsample_rates: [8, 8, 2, 2]
    upsample_kernel_sizes: [16, 16, 4, 4]
    resblock_kernel_sizes: [3, 7, 11]
    resblock_dilation_sizes: [[1,3,5], [1,3,5], [1,3,5]]
    gan_feat_matching_dim: 8
    multi_period_discriminator_periods: [2, 3, 5, 7, 11]

# Training config (reduced for faster iteration)
training:
  batch_size: 4
  num_workers: 2
  learning_rate: 0.0001
  weight_decay: 0.0001
  scheduler: "OneCycleLR"
  scheduler_params:
    pct_start: 0.1
    gamma: 0.99
    T_max: 100
  grad_clip_val: 1.0
  max_epochs: 100
  precision: 32  # Full precision for better debugging
  accumulate_grad_batches: 1
  
  # Loss weights
  loss_weights:
    mel_loss: 45.0
    feature_loss: 2.0
    adversarial_loss: 1.0
    f0_loss: 10.0
    duration_loss: 1.0
  
# Validation config
validation:
  batch_size: 4
  num_workers: 2
  val_check_interval: 100  # More frequent validation
  num_audio_samples: 2
  
# Data preprocessing config
preprocessing:
  audio:
    sample_rate: 22050
    n_fft: 1024
    hop_length: 256
    win_length: 1024
    n_mels: 80
    f_min: 0
    f_max: 8000
    
  text:
    g2p_model: "espeak"
    phoneme_dict: "data/phoneme_dict.json"
    cleaner: "english_cleaners"

  f0:
    min_f0: 65
    max_f0: 1000
    hop_length: 256
    voice_ranges:
      bass: [65, 300]
      baritone: [80, 350]
      tenor: [130, 500]
      alto: [160, 700]
      soprano: [200, 1100]

# Logger settings
logger:
  tensorboard:
    flush_secs: 60
    update_freq: "step"  # More frequent updates
    log_graph: true
    max_queue: 10
  
  # Audio logging settings
  audio:
    log_every_n_steps: 50  # More frequent audio logging
    sample_rate: 22050
    n_samples_per_batch: 1

# Inference settings
inference:
  batch_size: 1
  checkpoint_path: "best"
  output_dir: "outputs/single_speaker_test/generated"
  temperature: 1.0
  output_format: "wav"